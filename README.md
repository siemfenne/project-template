# Data Science Project Template

A standardized cookiecutter template for Data Science projects at Medtronic, designed to enforce best practices and provide a consistent project structure across teams.

## Quick Start 

### Prerequisites
- Cookiecutter installed (`pip install cookiecutter==2.6.0`)

### Generate a New Project

```bash
cookiecutter https://dev.azure.com/MedtronicBI/DigIC%20GR%20AI/_git/project-template-2
```

You'll be prompted to answer several questions that customize your project:

#### Configuration Prompts

| Prompt | Description | Options |
|--------|-------------|---------|
| `project_name` | Human-readable project name | Free text (e.g., "Yoda - Recommended Opportunity") |
| `repo_name` | Repository/folder name | Free text (e.g., "PXXX") |
| `author_name` | Author or organization name | Free text (e.g., "Medtronic") |
| `description` | Brief project description | Free text |
| `project_platform` | Target platform for deployment | `Snowflake` or `Databricks` |
| `python_version` | Python version for the project | Free text (e.g., "3.9", "3.10") |
| `environment_manager` | Python environment management tool | `virtualenv`, `conda`, `pipenv`, `poetry`, `uv`, `none` |
| `dependency_file` | Dependency specification format | `requirements.txt`, `pyproject.toml`, `environment.yml`, `Pipfile` |

## Project Structure

The generated project follows data science best practices with clear separation of concerns:

```
repo_name/
‚îú‚îÄ‚îÄ docs/                          # Project documentation
‚îÇ   ‚îú‚îÄ‚îÄ docs/                      # MkDocs documentation source
‚îÇ   ‚îú‚îÄ‚îÄ mkdocs.yml                 # MkDocs configuration
‚îÇ   ‚îî‚îÄ‚îÄ README.md                  # Project-specific README
‚îú‚îÄ‚îÄ models/                        # Trained models and serialized objects
‚îú‚îÄ‚îÄ notebooks/                     # Jupyter notebooks
‚îÇ   ‚îú‚îÄ‚îÄ requirements/              # Dependency files for different runtimes
‚îÇ   ‚îî‚îÄ‚îÄ shared/                    # Shared notebook utilities
‚îú‚îÄ‚îÄ references/                    # Data dictionaries, manuals, explanatory materials
‚îú‚îÄ‚îÄ reports/                       # Generated analysis reports
‚îú‚îÄ‚îÄ .gitignore                    # Git ignore rules
‚îú‚îÄ‚îÄ setup.ps1                     # Windows PowerShell setup script
‚îú‚îÄ‚îÄ setup.sh                      # Unix/macOS setup script
‚îî‚îÄ‚îÄ [platform-specific files]    # Files based on platform choice
```

### Platform and OS-Specific Files

The template automatically configures your project based on the selected platform and operating system using **post-generation hooks**. Unwanted platform-specific and OS-specific files are automatically removed during project generation.

#### When `project_platform = "Snowflake"`:
```
‚îú‚îÄ‚îÄ deploy/                       # Snowflake deployment configuration
‚îÇ   ‚îú‚îÄ‚îÄ config.toml              # Snowflake connection config
‚îÇ   ‚îú‚îÄ‚îÄ deploy.sql               # SQL deployment scripts (generated by pipeline)
‚îÇ   ‚îî‚îÄ‚îÄ deploy_sql.py            # Python script for SQL generation
‚îú‚îÄ‚îÄ apps/                        # Streamlit/web apps directory
‚îÇ   ‚îú‚îÄ‚îÄ main.py                  # Main application entry point
‚îÇ   ‚îî‚îÄ‚îÄ environment.yml          # App dependencies
‚îî‚îÄ‚îÄ azure-pipeline-sf.yml        # Azure DevOps CI/CD pipeline for Snowflake
```
*Note: Databricks-specific files (`azure-pipeline-dbx.yml`, `databricks.yml`) are automatically removed.*

#### When `project_platform = "Databricks"`:
```
‚îú‚îÄ‚îÄ databricks.yml               # Databricks project configuration for pipeline
‚îî‚îÄ‚îÄ azure-pipeline-dbx.yml       # Azure DevOps CI/CD pipeline template for Databricks
```
*Note: Snowflake-specific files (`deploy/`, `apps/`, `azure-pipeline-sf.yml`) are automatically removed.*

#### Setup Scripts
Both Windows and macOS/Linux scripts are included in every generated project:
```
‚îú‚îÄ‚îÄ setup.ps1                    # Windows PowerShell setup script
‚îú‚îÄ‚îÄ setup.sh                     # Unix/macOS setup script

```

### Dependency Management Files

Based on your `dependency_file` choice, one of the following will be created in the `notebooks/` directory:

- **requirements.txt**: Traditional pip requirements
- **pyproject.toml**: Modern Python packaging with Poetry/UV
- **environment.yml**: Conda environment specification
- **Pipfile**: Pipenv dependency management

## Getting Started with Your New Project

After generating your project:

> **üìù Note**: The template uses post-generation hooks to automatically clean up platform-specific files. You'll see confirmation messages during generation (e.g., "Removed azure-pipeline-sf.yml", "Project template configured for Databricks").

### 1. Setup Scripts (Recommended)

Navigate to your project directory and run the setup script for your operating system:

**Windows:**
```powershell
.\setup.ps1
```

**macOS/Linux:**
```bash
./setup.sh
```

This will:
- Configure Azure DevOps CLI with organization and project defaults
- Prompt for new repository name
- Create new Azure DevOps repository
- Initialize local git repository with initial commit
- Create and push `main`, `stage`, and `dev` branches
- Set up remote origin
- Optionally set up Snowflake integration:
  - Create Git repository object in Snowflake (linked to Azure DevOps)
  - Create project schema in `DEV_GR_AI_DB`
  - Grant appropriate permissions to `GR_AI_ENGINEER` role
- Optionally link repository in Databricks:
  - Create repository links in DEV environment
  - Set up workspace paths under user directory

**Important**: When you choose to set up Snowflake integration, you will be prompted to provide a private key passphrase. Here you have to insert the passphrase of the Service Principal. Please ask Ronald to pass it to you.

### 2. Working with Snowflake Workspaces

With Snowflake Workspaces, notebooks and Streamlit apps are now managed directly within Snowflake:

- **Notebooks**: Create and edit Jupyter notebooks directly in Snowflake Workspaces. They persist across git branches and don't require CI/CD deployment.
- **Native Streamlit Apps**: Create Streamlit apps directly in Snowflake Workspaces without needing containerization.
- **Containerized Streamlit Apps**: For apps requiring custom dependencies via Docker, use the CI/CD pipeline (see below).

## Branching Strategy

This template implements a Git branching strategy aligned with the team's deployment pipeline:

- **`main`** - Production branch ‚Üí `PROD_GR_AI_DB`
- **`stage`** - Staging branch ‚Üí `STAGE_GR_AI_DB`  
- **`dev`** - Development branch ‚Üí `DEV_GR_AI_DB`
- **`feature/*`** - Feature branches ‚Üí `DEV_GR_AI_DB`

## CI/CD Pipeline

The included pipeline file (platform-specific) provides:

- **Triggers**: Pushes to `main` and `stage` branches
- **Environment-specific database mapping** via Azure DevOps variables
- **Platform-specific authentication**:
  - **Snowflake**: JWT authentication via private key
  - **Databricks**: Token-based authentication
- **Git repository sync** (fetches latest from Azure DevOps to Snowflake)
- **Connection testing** and validation

### Snowflake Deployments

The Snowflake CI/CD pipeline automatically deploys:

#### Notebooks
All `.ipynb` files in the repository are deployed as Snowflake Notebooks:
- Creates notebook objects from the Git repository
- Adds live versions automatically
- Configures external access integrations (PyPI)

#### Streamlit Apps
Apps in the `apps/` folder are deployed based on their structure:

**Native Streamlit Apps** (no Dockerfile):
- Deployed directly as Snowflake Streamlit apps
- Entry point: `main.py` or `streamlit_app.py`

**Containerized Apps** (with Dockerfile):
- Built as Docker images and pushed to Snowflake's image registry
- Deployed as Snowpark Container Services
- Useful for apps requiring custom dependencies

To deploy a containerized app:
1. Create a subfolder under `apps/` (e.g., `apps/myapp/`)
2. Add your `main.py` or `streamlit_app.py`
3. Add a `Dockerfile`
4. (Optional) Add an `environment.yml` for Conda dependencies

## Best Practices

### Code Organization
- Use notebooks for exploration, analysis, and reporting
- Maintain shared utilities in `notebooks/shared/`
- Store trained models in `models/`

### Documentation
- Update project README in `docs/README.md`
- Use MkDocs for comprehensive documentation
- Document data schemas in `references/`

### Version Control
- Follow the branching strategy
- Use meaningful commit messages
- Create pull requests for code reviews
