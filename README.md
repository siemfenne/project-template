# Data Science Project Template

A standardized cookiecutter template for Data Science projects at Medtronic, designed to enforce best practices and provide a consistent project structure across teams.

## Quick Start

### Prerequisites
- Cookiecutter installed (`pip install cookiecutter==2.6.0`)

### Generate a New Project

```bash
cookiecutter https://dev.azure.com/MedtronicBI/DigIC%20GR%20AI/_git/project-template
```

You'll be prompted to answer several questions that customize your project:

#### Configuration Prompts

| Prompt | Description | Options |
|--------|-------------|---------|
| `project_name` | Human-readable project name | Free text (e.g., "Yoda - Recommended Opportunity") |
| `repo_name` | Repository/folder name | Free text (e.g., "PXXX") |
| `author_name` | Author or organization name | Free text (e.g., "Medtronic") |
| `description` | Brief project description | Free text |
| `project_platform` | Target platform for deployment | `Snowflake` or `Databricks` |
| `environment_manager` | Python environment management tool | `virtualenv`, `conda`, `pipenv`, `uv`, `none` |
| `dependency_file` | Dependency specification format | `requirements.txt`, `pyproject.toml`, `environment.yml`, `Pipfile` |

## Project Structure

The generated project follows data science best practices with clear separation of concerns:

```
repo_name/
â”œâ”€â”€ data/                          # Data storage (git-ignored)
â”‚   â”œâ”€â”€ external/                  # External data sources
â”‚   â”œâ”€â”€ interim/                   # Intermediate processed data
â”‚   â”œâ”€â”€ processed/                 # Final processed data
â”‚   â””â”€â”€ raw/                       # Raw immutable data
â”œâ”€â”€ docs/                          # Project documentation
â”‚   â”œâ”€â”€ docs/                      # MkDocs documentation source
â”‚   â”œâ”€â”€ mkdocs.yml                 # MkDocs configuration
â”‚   â””â”€â”€ README.md                  # Project-specific README
â”œâ”€â”€ models/                        # Trained models and serialized objects
â”œâ”€â”€ notebooks/                     # Jupyter notebooks
â”‚   â”œâ”€â”€ shared/                    # Shared notebook utilities
â”‚   â””â”€â”€ [dependency_file]          # Environment-specific dependency file
â”œâ”€â”€ references/                    # Data dictionaries, manuals, explanatory materials
â”œâ”€â”€ reports/                       # Generated analysis reports
â”œâ”€â”€ src/                          # Source code for the project
â”‚   â””â”€â”€ __init__.py               # Makes src a Python module
â”œâ”€â”€ .env                          # Environment variables (git-ignored)
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”œâ”€â”€ azure-pipeline.yml            # Azure DevOps CI/CD pipeline
â”œâ”€â”€ Makefile                      # Automation commands
â”œâ”€â”€ setup.cfg                     # Python package configuration
â”œâ”€â”€ setup.ps1                    # Windows setup script
â”œâ”€â”€ setup.sh                     # Unix setup script
â””â”€â”€ [platform-specific files]    # Files based on platform choice
```

### Platform-Specific Files

#### When `project_platform = "Snowflake"`:
```
â”œâ”€â”€ .snowflake/                   # Snowflake configuration
â”‚   â”œâ”€â”€ config.toml              # Snowflake connection config
â”‚   â”œâ”€â”€ deploy.sql               # SQL deployment scripts
â”‚   â””â”€â”€ deploy_sql.py            # Python deployment utilities
â””â”€â”€ streamlit/                   # Streamlit app directory
    â”œâ”€â”€ streamlit_app.py         # Main Streamlit application
    â””â”€â”€ environment.yml         # Streamlit dependencies
```

#### When `project_platform = "Databricks"`:
```
â””â”€â”€ databricks.yml               # Databricks project configuration for pipeline
```

### Dependency Management Files

Based on your `dependency_file` choice, one of the following will be created in the `notebooks/` directory:

- **requirements.txt**: Traditional pip requirements
- **pyproject.toml**: Modern Python packaging with Poetry/UV
- **environment.yml**: Conda environment specification
- **Pipfile**: Pipenv dependency management

## Getting Started with Your New Project

After generating your project:

### 1. Initial Setup

Navigate to your project directory and run the appropriate setup script:

**Windows (PowerShell):**
```powershell
.\setup.ps1
```

**Unix/Linux/macOS:**
```bash
./setup.sh
```

### 2. Initialize Repository (Optional)

Use the Makefile to automatically create an Azure DevOps repository and set up branching:

```bash
make init-repo
```

This will:
- Create a new Azure DevOps repository
- Initialize git with initial commit
- Create and push `main`, `stage`, and `dev` branches
- Set up remote origin

### 3. Available Make Commands

| Command | Description |
|---------|-------------|
| `make test` | Run basic project tests |
| `make init-repo` | Create Azure DevOps repo and initialize git |
| `make full` | Complete project setup |
| `make snowflake` | Snowflake-specific setup |
| `make databricks` | Databricks-specific setup |
| `make feature-branch` | Create a new feature branch |

## Branching Strategy

This template implements a Git branching strategy aligned with the team's deployment pipeline:

- **`main`** - Production branch â†’ `PROD_GR_AI_DB`
- **`stage`** - Staging branch â†’ `STAGE_GR_AI_DB`  
- **`dev`** - Development branch â†’ `DEV_GR_AI_DB`
- **`feature/*`** - Feature branches â†’ `DEV_GR_AI_DB`

## CI/CD Pipeline

The included `azure-pipeline.yml` provides:

- **Triggers**: Pull requests to stage/main and pushes to main/stage/dev
- **Environment-specific database mapping** via Azure DevOps variables
- **Snowflake CLI authentication** via JWT (private key)
- **Connection testing** for both temporary and named connections
- **Notebook structure validation**

## ğŸ“ Best Practices

### Data Management
- Store raw data in `data/raw/` (immutable)
- Process data through `data/interim/` to `data/processed/`
- Never commit data files to git (handled by `.gitignore`)

### Code Organization
- Keep reusable code in `src/`
- Use notebooks for exploration and reporting
- Maintain shared utilities in `notebooks/shared/`

### Documentation
- Update project README in `docs/README.md`
- Use MkDocs for comprehensive documentation
- Document data schemas in `references/`

### Version Control
- Follow the branching strategy
- Use meaningful commit messages
- Create pull requests for code reviews
