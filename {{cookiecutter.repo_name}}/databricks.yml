bundle: # bundle metadata, defines the overall project (you can use PXYZ for your project)
  name: {{cookiecutter.repo_name}}

variables: # global variables that can be reused in jobs (overridden per target)
  cluster_id: # each cluster in databricks has a unique id
    default: ""
  database: 
    default: ""
  schema: 
    default: ""

######################################################
########## EXAMPLE RESOURCE SECTION START ############
######################################################

# resources: # databricks resources to be created on deploy (jobs, pipelines, etc.)
#   jobs:
#     exc-data-refresh-job: # first job to refresh data
#       name: DEVOPSCLOUD_EXC_DATA_REFRESH_JOB
#       parameters: # job-level parameters (shared across tasks, overridable at runtime). A job can include multiple tasks that for example execute a notebook
#         - name: database
#           default: ${var.database}
#         - name: schema
#           default: ${var.schema}
#       tasks: # individual tasks within the job
#         - task_key: data_preparation # first task: prepares data
#           notebook_task:
#             notebook_path: ./notebooks/DEVOPSCLOUD_01_01_DATA_PREPARATION.ipynb # path to notebook in project
#             base_parameters: # task-specific parameters (in addition to job parameters)
#               dataset_type: INFER
#           existing_cluster_id: ${var.cluster_id}
#         - task_key: result_sets # second task: generates result sets
#           notebook_task:
#             notebook_path: ./notebooks/DEVOPSCLOUD_01_04_RESULT_SETS.ipynb
#           existing_cluster_id: ${var.cluster_id}
#           depends_on: # runs only after data_preparation finishes
#             - task_key: data_preparation
#       schedule: # cron-based job scheduling
#         quartz_cron_expression: "0 55 5 * * ?"
#         timezone_id: "Europe/Amsterdam"
#         pause_status: UNPAUSED

#     # ONE-TIME JOB that is executed on deploy to initialize tables and views in the database
#     database-init-job: # IMPORTANT: This will only be executed if you define it in the azure-pipelines.yml file (see databricks.exe bundle run -t $env:BUNDLE_TARGET database-init-job)
#       name: DEVOPSCLOUD_DATABASE_INIT
#       parameters:
#         - name: database
#           default: ${var.database}
#         - name: schema
#           default: ${var.schema}
#       tasks:
#         - task_key: database_init
#           notebook_task:
#             notebook_path: ./notebooks/DEVOPSCLOUD_01_00_DATABASE_INIT.ipynb
#           existing_cluster_id: ${var.cluster_id}

######################################################
########### EXAMPLE RESOURCE SECTION END #############
######################################################

targets: # environment-specific overrides (stage/prod)
  dev:
    workspace: # databricks workspace for dev
      host: "https://medtronic-ml-globalregionsit-dev.cloud.databricks.com"
      profile: "dev"
      root_path: "/Workspace/Users/${workspace.current_user.userName}/dev_sandbox/{{cookiecutter.repo_name}}"

  stage: 
    workspace: # databricks workspace for stage
      host: "https://medtronic-ml-globalregionsit-stage.cloud.databricks.com"
    variables: # stage-specific values for variables
      cluster_id: "<cluster_id_prod>"
      database: "PROD_GR_AI_DB"
      schema: "{{cookiecutter.repo_name}}"

  prod:
    workspace: # databricks workspace for prod
      host: "https://medtronic-ml-globalregionsit-prod.cloud.databricks.com"
    variables: # prod-specific values for variables
      cluster_id: "<cluster_id_prod>"
      database: "PROD_GR_AI_DB"
      schema: "{{cookiecutter.repo_name}}"